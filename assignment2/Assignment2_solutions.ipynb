{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/janchorowski/nn_assignments/blob/nn18/assignment2/Assignment2.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C99k_YdbQ5Gy"
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "**Updated Submission deadline:**\n",
    "- **Problems 1-4: last lab session before or on Friday 26.10.18**\n",
    "- **Problems 5-8: last lab session before or on Friday 01.11.18**\n",
    "\n",
    "**Points: 16 + 3 bonus points**\n",
    "\n",
    "\n",
    "## Downloading this notebook\n",
    "\n",
    "This assignment is an Colab/Jupyter notebook. Download it by cloning https://github.com/janchorowski/nn_assignments or open from Colab. Follow the instructions in its README for instructions. Whenever possible, add your solutions to the notebook.\n",
    "\n",
    "Please email us about any problems with it - we will try to correct them quickly. Also, please do not hesitate to use GitHub’s pull requests to send us corrections!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww7CDGWZQ5G1"
   },
   "source": [
    "# Problem 1 [2p] Bayes' Theorem\n",
    "\n",
    "Bayes' theorem allows to reason about conditional probabilities of causes and their effects:\n",
    "\n",
    "\\begin{equation}\n",
    "p(A,B)=p(A|B)p(B)=p(B|A)p(A)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "p(A|B) = \\frac{p(B|A)p(A)}{p(B)}\n",
    "\\end{equation}\n",
    "\n",
    "Bayes' theorem allows us to reason about probabilities of causes, when\n",
    "we observe their results.  Instead of directly answering the hard\n",
    "question $p(\\text{cause}|\\text{result})$ we can instead separately\n",
    "work out the marginal probabilities of causes $p(\\text{cause})$ and\n",
    "carefully study their effects $p(\\text{effect}|\\text{cause})$.\n",
    "\n",
    "Solve the following using Bayes' theorem.\n",
    "\n",
    "1. **[1p]** There are two boxes on the table: box \\#1 holds two\n",
    "  black balls and eight red ones, box \\#2 holds 5 black ones and\n",
    "  5 red ones. We pick a box at random (with equal probabilities),\n",
    "  and then a ball from that box.\n",
    "  1. What is the probability, that the\n",
    "  ball came from box \\#1 if we happened to pick a red ball?\n",
    "  \n",
    "1. **[1p]** The government has started a preventive program of\n",
    "  mandatory tests for the Ebola virus. Mass testing method is\n",
    "  imprecise, yielding 1% of false positives (healthy, but the test\n",
    "  indicates the virus) and 1% of false negatives (\n",
    "  having the virus but healthy according to test results).\n",
    "  As Ebola is rather infrequent, lets assume that it occurs in\n",
    "  one in a million people in Europe.\n",
    "  1. What is the probability,\n",
    "  that a random European, who has been tested positive for Ebola\n",
    "  virus, is indeed a carrier?\n",
    "  2. Suppose we have an additional information, that the person has just\n",
    "  arrived from a country where one in a thousand people is a carrier.\n",
    "  How much will be the increase in probability?\n",
    "  3. How accurate should be the test, for a 80% probability of true\n",
    "  positive in a European?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pKUFimAQQ5G3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.A) 0.6153846153846154\n"
     ]
    }
   ],
   "source": [
    "# TODO Fill in the calculations instead of doing them by hand\n",
    "# TODO Be precise and remember to add comments!\n",
    "\n",
    "box_1_black = 2.\n",
    "box_1_red = 8.\n",
    "box_1 = box_1_black + box_1_red\n",
    "box_2_black = 5.\n",
    "box_2_red = 5.\n",
    "box_2 = box_2_black + box_2_red\n",
    "total_box = box_1 + box_2\n",
    "\n",
    "p_box_1 = box_1 / total_box\n",
    "p_red = (box_1_red + box_2_red) / total_box\n",
    "p_red_if_box_1 = box_1_red / box_1\n",
    "p_box_1_if_red = p_red_if_box_1 * p_box_1 / p_red\n",
    "\n",
    "print('1.A) %s' % p_box_1_if_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.A) European carrier if positive: 9.89903e-05\n",
      "2.B) Foreign carrier if positive: 0.0901639\tdifference: 0.0900649\n",
      "2.C) Accuracy: 0.999999750\n"
     ]
    }
   ],
   "source": [
    "p_false_positive = p_positive_if_healthy = 0.01\n",
    "p_true_positive = p_positive_if_carrier = 1 - p_false_positive\n",
    "p_false_negative = p_negative_if_carrier = 0.01\n",
    "p_true_negative = p_negative_if_healthy = 1 - p_false_negative\n",
    "\n",
    "\n",
    "def p_carrier_if_positive(p_carrier):\n",
    "    p_healthy = 1 - p_carrier\n",
    "    p_positive = p_positive_if_carrier * p_carrier + p_positive_if_healthy * p_healthy\n",
    "\n",
    "    p_carrier_if_positive = p_positive_if_carrier * p_carrier / p_positive\n",
    "    return p_carrier_if_positive\n",
    "\n",
    "# 2.A)\n",
    "p_european_carrier = 1e-6\n",
    "p_european_carrier_if_positive = p_carrier_if_positive(p_european_carrier)\n",
    "print('2.A) European carrier if positive: %g' % p_european_carrier_if_positive)\n",
    "\n",
    "# 2.B)\n",
    "p_foreign_carrier = 1e-3\n",
    "p_foreign_carrier_if_positive = p_carrier_if_positive(p_foreign_carrier)\n",
    "print('2.B) Foreign carrier if positive: %g\\tdifference: %g' % (\n",
    "    p_foreign_carrier_if_positive,\n",
    "    p_foreign_carrier_if_positive - p_european_carrier_if_positive)) \n",
    "\n",
    "# 2.C\n",
    "def ex2C():\n",
    "    p_carrier = p_european_carrier\n",
    "    p_positive_if_carrier = 0.8\n",
    "    \n",
    "    accuracy = (\n",
    "        p_positive_if_carrier * (p_carrier - 1)\n",
    "        / (2 * p_positive_if_carrier * p_carrier - p_positive_if_carrier - p_carrier)\n",
    "    )\n",
    "    return accuracy\n",
    "\n",
    "print('2.C) Accuracy: %.9f' % ex2C())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bJn8c_uwQ5G9"
   },
   "source": [
    "# Problem 2 [2p + 1b] Naive Bayes Classifier\n",
    "\n",
    "The Bayes' theorem allows us to construct a classifier in which we\n",
    "model how the data is generated. Here we will describe a\n",
    "simple and popular example of such a classifier called the naive\n",
    "Bayes classifier.  Despite its simplicity It is quite effective for\n",
    "classification of text documents (e.g. as spam and non-spam).\n",
    "\n",
    "Let a document be a sequence of words $D=W_1,W_2,\\ldots,W_n$ \n",
    "We will model generation of text documents as a two-stage process.\n",
    "First, document category $C_j$ is drawn at random with probability\n",
    "$p(C_j)$, also called the *a priori* probability.\n",
    "To define the class-conditional probability\n",
    "$p(D|C_j)$, we will make a simplifying (naive)\n",
    "assumption, that every word in the document is drawn independently at\n",
    "random with probability $p(W_i|C)$:\n",
    "\n",
    "\\begin{equation*}\n",
    "  p(D|C_j) = p(W_1,W_2,\\ldots,W_n | C_j) \\approx p(W_1|C_j)p(W_2|C_j)\\ldots p(W_n|C_j).\n",
    "\\end{equation*}\n",
    "\n",
    "To infer the class of a document we apply the Bayes theorem:\n",
    "\\begin{equation*}   p(C_j|D) = \\frac{p(D|C_j)p(C_j)}{p(D)} = \\frac{p(C_j)p(W_1|C_j)p(W_2|C_j)\\ldots p(W_n|C_j)}{p(D)}.\n",
    "\\end{equation*}\n",
    "Please note that since we assumed only a finite number of classes,\n",
    "we can compute the term $p(D)$ by making sure that the *a\n",
    "posteriori probabilities* $p(C_j|D)$ sum to $1$ over all classes.\n",
    "\n",
    "In this exercise we will try to mimic the language-guessing feature\n",
    "of [Google Translate](https://translate.google.com/), although\n",
    "on a much smaller scale.  We are given an input which is a\n",
    "lower-case sequence of characters (such as *\"some people like\n",
    "pineapple on their pizza\"*), and we determine whether the\n",
    "sequence's language is English, Polish or Spanish.\n",
    "We will treat each character as a separate observation.\n",
    "The numbers are taken from [Wikipedia article on letter frequency](https://en.wikipedia.org/wiki/Letter_frequency#Relative_frequencies_of_letters_in_other_languages). We display the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kpHvgJRwQ5G-",
    "outputId": "908125b8-b50f-46f1-bb18-04a1c8a8a88d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "      <th>German</th>\n",
       "      <th>Spanish</th>\n",
       "      <th>Portuguese</th>\n",
       "      <th>Esperanto</th>\n",
       "      <th>Italian</th>\n",
       "      <th>Turkish</th>\n",
       "      <th>Swedish</th>\n",
       "      <th>Polish</th>\n",
       "      <th>Dutch</th>\n",
       "      <th>Danish</th>\n",
       "      <th>Icelandic</th>\n",
       "      <th>Finnish</th>\n",
       "      <th>Czech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>8.167</td>\n",
       "      <td>7.636</td>\n",
       "      <td>6.516</td>\n",
       "      <td>11.525</td>\n",
       "      <td>14.634</td>\n",
       "      <td>12.117</td>\n",
       "      <td>11.745</td>\n",
       "      <td>12.920</td>\n",
       "      <td>9.383</td>\n",
       "      <td>10.503</td>\n",
       "      <td>7.486</td>\n",
       "      <td>6.025</td>\n",
       "      <td>10.110</td>\n",
       "      <td>12.217</td>\n",
       "      <td>8.421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>1.492</td>\n",
       "      <td>0.901</td>\n",
       "      <td>1.886</td>\n",
       "      <td>2.215</td>\n",
       "      <td>1.043</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.927</td>\n",
       "      <td>2.844</td>\n",
       "      <td>1.535</td>\n",
       "      <td>1.740</td>\n",
       "      <td>1.584</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.043</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>2.782</td>\n",
       "      <td>3.260</td>\n",
       "      <td>2.732</td>\n",
       "      <td>4.019</td>\n",
       "      <td>3.882</td>\n",
       "      <td>0.776</td>\n",
       "      <td>4.501</td>\n",
       "      <td>1.463</td>\n",
       "      <td>1.486</td>\n",
       "      <td>3.895</td>\n",
       "      <td>1.242</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>4.253</td>\n",
       "      <td>3.669</td>\n",
       "      <td>5.076</td>\n",
       "      <td>5.010</td>\n",
       "      <td>4.992</td>\n",
       "      <td>3.044</td>\n",
       "      <td>3.736</td>\n",
       "      <td>5.206</td>\n",
       "      <td>4.702</td>\n",
       "      <td>3.725</td>\n",
       "      <td>5.933</td>\n",
       "      <td>5.858</td>\n",
       "      <td>1.575</td>\n",
       "      <td>1.043</td>\n",
       "      <td>3.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>12.702</td>\n",
       "      <td>14.715</td>\n",
       "      <td>16.396</td>\n",
       "      <td>12.181</td>\n",
       "      <td>12.570</td>\n",
       "      <td>8.995</td>\n",
       "      <td>11.792</td>\n",
       "      <td>9.912</td>\n",
       "      <td>10.149</td>\n",
       "      <td>7.352</td>\n",
       "      <td>18.910</td>\n",
       "      <td>15.453</td>\n",
       "      <td>6.418</td>\n",
       "      <td>7.968</td>\n",
       "      <td>7.562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   English  French  German  Spanish  Portuguese  Esperanto  Italian  Turkish  \\\n",
       "a    8.167   7.636   6.516   11.525      14.634     12.117   11.745   12.920   \n",
       "b    1.492   0.901   1.886    2.215       1.043      0.980    0.927    2.844   \n",
       "c    2.782   3.260   2.732    4.019       3.882      0.776    4.501    1.463   \n",
       "d    4.253   3.669   5.076    5.010       4.992      3.044    3.736    5.206   \n",
       "e   12.702  14.715  16.396   12.181      12.570      8.995   11.792    9.912   \n",
       "\n",
       "   Swedish  Polish   Dutch  Danish  Icelandic  Finnish  Czech  \n",
       "a    9.383  10.503   7.486   6.025     10.110   12.217  8.421  \n",
       "b    1.535   1.740   1.584   2.000      1.043    0.281  0.822  \n",
       "c    1.486   3.895   1.242   0.565      0.000    0.281  0.740  \n",
       "d    4.702   3.725   5.933   5.858      1.575    1.043  3.475  \n",
       "e   10.149   7.352  18.910  15.453      6.418    7.968  7.562  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "wiki_table = u\"\"\"English|French|German|Spanish|Portuguese|Esperanto|Italian|Turkish|Swedish|Polish|Dutch|Danish|Icelandic|Finnish|Czech\\na|8.167|7.636|6.516|11.525|14.634|12.117|11.745|12.920|9.383|10.503|7.486|6.025|10.110|12.217|8.421\\nb|1.492|0.901|1.886|2.215|1.043|0.980|0.927|2.844|1.535|1.740|1.584|2.000|1.043|0.281|0.822\\nc|2.782|3.260|2.732|4.019|3.882|0.776|4.501|1.463|1.486|3.895|1.242|0.565|0|0.281|0.740\\nd|4.253|3.669|5.076|5.010|4.992|3.044|3.736|5.206|4.702|3.725|5.933|5.858|1.575|1.043|3.475\\ne|12.702|14.715|16.396|12.181|12.570|8.995|11.792|9.912|10.149|7.352|18.91|15.453|6.418|7.968|7.562\\nf|2.228|1.066|1.656|0.692|1.023|1.037|1.153|0.461|2.027|0.143|0.805|2.406|3.013|0.194|0.084\\ng|2.015|0.866|3.009|1.768|1.303|1.171|1.644|1.253|2.862|1.731|3.403|4.077|4.241|0.392|0.092\\nh|6.094|0.737|4.577|0.703|0.781|0.384|0.636|1.212|2.090|1.015|2.380|1.621|1.871|1.851|1.356\\ni|6.966|7.529|6.550|6.247|6.186|10.012|10.143|9.600|5.817|8.328|6.499|6.000|7.578|10.817|6.073\\nj|0.153|0.613|0.268|0.493|0.397|3.501|0.011|0.034|0.614|1.836|1.46|0.730|1.144|2.042|1.433\\nk|0.772|0.049|1.417|0.011|0.015|4.163|0.009|5.683|3.140|2.753|2.248|3.395|3.314|4.973|2.894\\nl|4.025|5.456|3.437|4.967|2.779|6.104|6.510|5.922|5.275|2.564|3.568|5.229|4.532|5.761|3.802\\nm|2.406|2.968|2.534|3.157|4.738|2.994|2.512|3.752|3.471|2.515|2.213|3.237|4.041|3.202|2.446\\nn|6.749|7.095|9.776|6.712|4.446|7.955|6.883|7.987|8.542|6.237|10.032|7.240|7.711|8.826|6.468\\no|7.507|5.796|2.594|8.683|9.735|8.779|9.832|2.976|4.482|6.667|6.063|4.636|2.166|5.614|6.695\\np|1.929|2.521|0.670|2.510|2.523|2.755|3.056|0.886|1.839|2.445|1.57|1.756|0.789|1.842|1.906\\nq|0.095|1.362|0.018|0.877|1.204|0|0.505|0|0.020|0|0.009|0.007|0|0.013|0.001\\nr|5.987|6.693|7.003|6.871|6.530|5.914|6.367|7.722|8.431|5.243|6.411|8.956|8.581|2.872|4.799\\ns|6.327|7.948|7.270|7.977|6.805|6.092|4.981|3.014|6.590|5.224|3.73|5.805|5.630|7.862|5.212\\nt|9.056|7.244|6.154|4.632|4.336|5.276|5.623|3.314|7.691|2.475|6.79|6.862|4.953|8.750|5.727\\nu|2.758|6.311|4.166|2.927|3.639|3.183|3.011|3.235|1.919|2.062|1.99|1.979|4.562|5.008|2.160\\nv|0.978|1.838|0.846|1.138|1.575|1.904|2.097|0.959|2.415|0.012|2.85|2.332|2.437|2.250|5.344\\nw|2.360|0.074|1.921|0.017|0.037|0|0.033|0|0.142|5.813|1.52|0.069|0|0.094|0.016\\nx|0.150|0.427|0.034|0.215|0.253|0|0.003|0|0.159|0.004|0.036|0.028|0.046|0.031|0.027\\ny|1.974|0.128|0.039|1.008|0.006|0|0.020|3.336|0.708|3.206|0.035|0.698|0.900|1.745|1.043\\nz|0.074|0.326|1.134|0.467|0.470|0.494|1.181|1.500|0.070|4.852|1.39|0.034|0|0.051|1.503\\nà|0|0.486|0|0|0.072|0|0.635|0|0|0|0|0|0|0|0\\nâ|0|0.051|0|0|0.562|0|0|0|0|0|0|0|0|0|0\\ná|0|0|0|0.502|0.118|0|0|0|0|0|0|0|1.799|0|0.867\\nå|0|0|0|0|0|0|0|0|1.338|0|0|1.190|0|0.003|0\\nä|0|0|0.578|0|0|0|0|0|1.797|0|0|0|0|3.577|0\\nã|0|0|0|0|0.733|0|0|0|0|0|0|0|0|0|0\\ną|0|0|0|0|0|0|0|0|0|0.699|0|0|0|0|0\\næ|0|0|0|0|0|0|0|0|0|0|0|0.872|0.867|0|0\\nœ|0|0.018|0|0|0|0|0|0|0|0|0|0|0|0|0\\nç|0|0.085|0|0|0.530|0|0|1.156|0|0|0|0|0|0|0\\nĉ|0|0|0|0|0|0.657|0|0|0|0|0|0|0|0|0\\nć|0|0|0|0|0|0|0|0|0|0.743|0|0|0|0|0\\nč|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.462\\nď|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.015\\nð|0|0|0|0|0|0|0|0|0|0|0|0|4.393|0|0\\nè|0|0.271|0|0|0|0|0.263|0|0|0|0|0|0|0|0\\né|0|1.504|0|0.433|0.337|0|0|0|0|0|0|0|0.647|0|0.633\\nê|0|0.218|0|0|0.450|0|0|0|0|0|0|0|0|0|0\\në|0|0.008|0|0|0|0|0|0|0|0|0|0|0|0|0\\nę|0|0|0|0|0|0|0|0|0|1.035|0|0|0|0|0\\ně|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1.222\\nĝ|0|0|0|0|0|0.691|0|0|0|0|0|0|0|0|0\\nğ|0|0|0|0|0|0|0|1.125|0|0|0|0|0|0|0\\nĥ|0|0|0|0|0|0.022|0|0|0|0|0|0|0|0|0\\nî|0|0.045|0|0|0|0|0|0|0|0|0|0|0|0|0\\nì|0|0|0|0|0|0|0.030|0|0|0|0|0|0|0|0\\ní|0|0|0|0.725|0.132|0|0|0|0|0|0|0|1.570|0|1.643\\nï|0|0.005|0|0|0|0|0|0|0|0|0|0|0|0|0\\nı|0|0|0|0|0|0|0|5.114|0|0|0|0|0|0|0\\nĵ|0|0|0|0|0|0.055|0|0|0|0|0|0|0|0|0\\nł|0|0|0|0|0|0|0|0|0|2.109|0|0|0|0|0\\nñ|0|0|0|0.311|0|0|0|0|0|0|0|0|0|0|0\\nń|0|0|0|0|0|0|0|0|0|0.362|0|0|0|0|0\\nň|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.007\\nò|0|0|0|0|0|0|0.002|0|0|0|0|0|0|0|0\\nö|0|0|0.443|0|0|0|0|0.777|1.305|0|0|0|0.777|0.444|0\\nô|0|0.023|0|0|0.635|0|0|0|0|0|0|0|0|0|0\\nó|0|0|0|0.827|0.296|0|0|0|0|1.141|0|0|0.994|0|0.024\\nõ|0|0|0|0|0.040|0|0|0|0|0|0|0|0|0|0\\nø|0|0|0|0|0|0|0|0|0|0|0|0.939|0|0|0\\nř|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.380\\nŝ|0|0|0|0|0|0.385|0|0|0|0|0|0|0|0|0\\nş|0|0|0|0|0|0|0|1.780|0|0|0|0|0|0|0\\nś|0|0|0|0|0|0|0|0|0|0.814|0|0|0|0|0\\nš|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.688\\nß|0|0|0.307|0|0|0|0|0|0|0|0|0|0|0|0\\nť|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.006\\nþ|0|0|0|0|0|0|0|0|0|0|0|0|1.455|0|0\\nù|0|0.058|0|0|0|0|0.166|0|0|0|0|0|0|0|0\\nú|0|0|0|0.168|0.207|0|0|0|0|0|0|0|0.613|0|0.045\\nû|0|0.060|0|0|0|0|0|0|0|0|0|0|0|0|0\\nŭ|0|0|0|0|0|0.520|0|0|0|0|0|0|0|0|0\\nü|0|0|0.995|0.012|0.026|0|0|1.854|0|0|0|0|0|0|0\\nů|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.204\\ný|0|0|0|0|0|0|0|0|0|0|0|0|0.228|0|0.995\\nź|0|0|0|0|0|0|0|0|0|0.078|0|0|0|0|0\\nż|0|0|0|0|0|0|0|0|0|0.706|0|0|0|0|0\\nž|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.721\"\"\"\n",
    "df = pd.read_table(StringIO(wiki_table), sep='|', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UhHSbS9cQ5HE"
   },
   "source": [
    "Implement the language classifier and answer the following:\n",
    "\n",
    "1. **[0.5p]** Naive Bayes can be implemented\n",
    "    either by multiplying probabilities or by adding\n",
    "    log-probabilities. Which one is better and why?\n",
    "2. **[1.5p]** What is the language of the following phrases, according to the classifier (below in a code cell)? Assume equal prior language probabilities $P(C)$.\n",
    "3. **[0-1 bonus]** What happens when a Naive Bayes classifier\n",
    "      is applied to a document with out-of-vocabulary words? Propose\n",
    "      some solutions. Relate them to the concept of Bayesian\n",
    "      priors discussed during the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ad.1. Summing many probabilities can lead to issues with precision in the case of the `multiplication`. The `log-pobabilities` is much more resistant to such problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ad.3. If we encounter `out-of-vocabulary` word we will have a problem with _mocking_ its probability.\n",
    "We can:\n",
    "- ignore it, especialy if we use the `log-probabilities`,\n",
    "- compute some `penalty-probability` (especially necessary in the case of the `multiplicative-probability`),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "03b7PdtRQ5HF"
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    u\"No dejes para mañana lo que puedas hacer hoy.\",\n",
    "    u\"Przed wyruszeniem w drogę należy zebrać drużynę.\",\n",
    "    u\"Żeby zrozumieć rekurencję, należy najpierw zrozumieć rekurencję.\",\n",
    "    u\"Si vale la pena hacerlo vale la pena hacerlo bien.\",\n",
    "    u\"Experience is what you get when you didn't get what you wanted.\",\n",
    "    u\"Należy prowokować intelekt, nie intelektualistów.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AFbrsRqIQ5HK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: English,French,German,Spanish,Portuguese,Esperanto,Italian,Turkish,Swedish,Polish,Dutch,Danish,Icelandic,Finnish,Czech\n",
      "Letters: a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, à, â, á, å, ä, ã, ą, æ, œ, ç, ĉ, ć, č, ď, ð, è, é, ê, ë, ę, ě, ĝ, ğ, ĥ, î, ì, í, ï, ı, ĵ, ł, ñ, ń, ň, ò, ö, ô, ó, õ, ø, ř, ŝ, ş, ś, š, ß, ť, þ, ù, ú, û, ŭ, ü, ů, ý, ź, ż, ž\n",
      "P(ę|Polish) = 1.035\n",
      "[0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667]\n",
      "Spanish : No dejes para mañana lo que puedas hacer hoy.\n",
      "Polish : Przed wyruszeniem w drogę należy zebrać drużynę.\n",
      "Polish : Żeby zrozumieć rekurencję, należy najpierw zrozumieć rekurencję.\n",
      "Italian : Si vale la pena hacerlo vale la pena hacerlo bien.\n",
      "English : Experience is what you get when you didn't get what you wanted.\n",
      "Polish : Należy prowokować intelekt, nie intelektualistów.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# We can easily access the data.\n",
    "langs = list(df)\n",
    "letters = list(df.index)\n",
    "print('Languages:', ','.join(langs))\n",
    "print('Letters:', ', '.join(letters))\n",
    "print('P(ę|Polish) =', df.loc['ę', 'Polish'])\n",
    "\n",
    "# Normalize values to a probability distribution\n",
    "df_norm = df.divide(df.values.sum(0) * len(langs))\n",
    "print(df_norm.values.sum(0))\n",
    "\n",
    "MIN_VALUE = df_norm.values[df_norm.values > 0].min() / 10\n",
    "LETTERS_SET = set(letters)\n",
    "\n",
    "\n",
    "def get_cleaned_letters(text):\n",
    "    return LETTERS_SET.intersection(set(text))\n",
    "\n",
    "\n",
    "def naive_bayes(sent, langs, df_norm):\n",
    "    \"\"\"Returns the most probable language of a sentence\"\"\"\n",
    "    probabilities = np.zeros(len(langs))\n",
    "    sent_letters = get_cleaned_letters(sent)\n",
    "\n",
    "    for letter in sent_letters:\n",
    "        letter_probs = df_norm.loc[letter, langs]\n",
    "        # If letter doesn't occur in the language (== 0) the \"log(0)\"\n",
    "        # would return \"-inf\". Changing it to a pre-computed \"MIN_VALUE\"\n",
    "        letter_probs[letter_probs == 0] = MIN_VALUE\n",
    "        probabilities += np.log(letter_probs)\n",
    "\n",
    "    return np.argmax(probabilities)\n",
    "\n",
    "\n",
    "for sent in sentences:\n",
    "    print(naive_bayes(sent, langs, df_norm), ':', sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QSwWrzzEQ5HO"
   },
   "source": [
    "# Problem 3 [2p]\n",
    "Given observations $x_1,\\ldots,x_n$\n",
    "  coming from a certain distribution,\n",
    "  prove that MLE of a particular parameter of that distribution is equal to the sample mean $\\frac{1}{n}\\sum_{i=1}^n x_i$:\n",
    "1. Bernoulli distribution with success probability $p$ and MLE $\\hat{p}$,\n",
    "2. Gaussian distribution $\\mathcal{N}(\\mu,\\sigma)$ and MLE $\\hat{\\mu}$,\n",
    "3. Poisson distribution $\\mathit{Pois}(\\lambda)$ and MLE $\\hat{\\lambda}$.\n",
    "\n",
    "*NOTE: You can submit your solution on paper. To freeze the solution and spend late days, at minimum send a photo of your solution via e-mail.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7u2TchcaRaaZ"
   },
   "source": [
    "# Problem 4: Simple Kalman filtering [3p + 2b]\n",
    "\n",
    "Oh no, someone has kidnapped you! You feel that you are in the trunk of a moving car. Luckily, you have your phone with GPS. Unfortunately, the GPS is noisy. You want to combine your estimate of your location by combining your prior belief about where you can be with the noisy GPS. You set out to implement a [1D Kalman filter](https://en.wikipedia.org/wiki/Kalman_filter).\n",
    "\n",
    "Problem setup:\n",
    "- your prior belief about the location is a Gaussian with mean 0 and some initial standard deviation $\\mathcal{N}(0, \\sigma_i)$\n",
    "- the car moves in a brownian motion - each time step, it changes location by a normally distributed random amound sampled from $\\mathcal{N}(0, \\sigma_m)$\n",
    "- each time step, you get a GPS reading which is sampled around your true (and sadly unknown to you ) location from $\\mathcal{N}(\\text{true loc}, \\sigma_g)$\n",
    "\n",
    "You want to use the following algorithm to track your location:\n",
    "\n",
    "1. Initially, the PDF of your location is $p(x) = \\mathcal{N}(x; \\mu_l=0, \\sigma_l=\\sigma_i)$\n",
    "2. For each time step, you update your belief about your location:\n",
    "    1. $p(x)$ is updated due to according to the car movement distribution\n",
    "    2. you use the Bayes formula to incorporate the GPS readout:\n",
    "       $$\n",
    "      p(x|\\text{GPS readout}) = \\frac{p(\\text{GPS readout}|x)p(x)}\n",
    "                                                           {p(\\text{GPS readout})}\n",
    "      $$\n",
    "    3. you set $p(x) \\gets p(x|\\text{GPS readout})$ to be your prior belief about your locatin used during the next iteration.\n",
    "\n",
    "\n",
    "NB: the GPS is actually very noisy, and Kalman filters are routinely used to fuse information from the GPS, accelerometers and odometry in practical applications, such as GPS navigation.\n",
    "\n",
    "#### Problem [1p]\n",
    "\n",
    "What is thePDF of\n",
    "$$\n",
    "p(x) = \\mathcal{N}(x;\\mu_1, \\sigma_1)\\mathcal{N}(x;\\mu_2, \\sigma_2)\n",
    "$$\n",
    "\n",
    "Hint: what disrtibution will the PDF belong to? Maybe you can simply compute the new mean and standard deviation?\n",
    "\n",
    "#### Problem [.5p]\n",
    "\n",
    "Implement below a simulator for your kidnapping, then fill in the code for plotting the true location and GPS readouts over time.\n",
    "\n",
    "#### Problem [1.5p]\n",
    "\n",
    "Implement a 1D Kalman filer using the algorithm stated above: maintian a probability distribution over your location, then at each timestep update it to account for car movement and GPS readouts.\n",
    "\n",
    "Plot the estimated location along with its standard deviation against the true location from the simluator.\n",
    "\n",
    "Experiemt with different setting for the standard deviations of the car's motion and the GPS. What happens if the simulator and the Kalman filter use different probability distributions?\n",
    "\n",
    "#### Problem [2p bonus]\n",
    "\n",
    "Suppose the car has a velocity, which is updated at each time step:\n",
    "$$\n",
    "\\begin{split}\n",
    "v &\\gets v + \\mathcal{N}(0, \\sigma_v) \\\\\n",
    "x &\\gets x + v \\\\\n",
    "\\text{GPS readout} &= \\mathcal{N}(x, \\sigma_g) \n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Update the Kalman filter code to track both the car's location and velocity. You cna assume that the initial velocity is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jUWXu0G_RssR"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vAESWWo0R224"
   },
   "outputs": [],
   "source": [
    "def simulate(initial_sigma, motion_sigma, gps_sigma, n_steps):\n",
    "    # This is the true (unknonw) location\n",
    "    x = norm.rvs(0, initial_sigma)\n",
    "    loc_hist = []\n",
    "    for s in range(n_steps):\n",
    "        #\n",
    "        # TODO: fill in a simulation step\n",
    "        #\n",
    "        loc_hist.append((x, gps_readout))\n",
    "    loc_df = pd.DataFrame(loc_hist, columns=['x', 'gps'])\n",
    "    return loc_df\n",
    "\n",
    "\n",
    "def kalman_predict(loc_df, initial_sigma, motion_sigma, gps_sigma):\n",
    "    prior_mu = 0\n",
    "    prior_sigma = initial_sigma\n",
    "    predictions = []\n",
    "    for gps_readout in loc_df.gps:\n",
    "        # expand the prior by the movement \n",
    "        #\n",
    "        # TODO: fill in\n",
    "        #\n",
    "        \n",
    "        # now do the bayes update\n",
    "        #\n",
    "        # TODO: fill in a simulation step\n",
    "        #\n",
    "        \n",
    "        predictions.append((posterior_mu, posterior_sigma))\n",
    "        prior_mu = posterior_mu\n",
    "        prior_sigma = posterior_sigma\n",
    "        \n",
    "\n",
    "    predictions_df = pd.DataFrame(predictions, columns=['mu', 'sigma'])\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "id": "YSMerNceR85F",
    "outputId": "0f2cb0b6-5cbd-4902-c603-0fb9bf4f470d"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gps_readout' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-92e0de18695c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mloc_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_sigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmotion_sigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgps_sigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mpredictions_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkalman_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_sigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmotion_sigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgps_sigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true position'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-e5108a5589eb>\u001b[0m in \u001b[0;36msimulate\u001b[0;34m(initial_sigma, motion_sigma, gps_sigma, n_steps)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# TODO: fill in a simulation step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloc_hist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgps_readout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mloc_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloc_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gps_readout' is not defined"
     ]
    }
   ],
   "source": [
    "#@title Parametry symulacji\n",
    "initial_sigma = 100 #@param\n",
    "motion_sigma = 4 #@param\n",
    "gps_sigma = 5 #@param\n",
    "n_steps = 50 #@param\n",
    "\n",
    "\n",
    "loc_df = simulate(initial_sigma, motion_sigma, gps_sigma, n_steps)\n",
    "predictions_df = kalman_predict(loc_df, initial_sigma, motion_sigma, gps_sigma)\n",
    "plt.plot(loc_df.x, 'r', label='true position')\n",
    "plt.plot(loc_df.gps, 'go', label='gps readout')\n",
    "plt.plot(predictions_df.mu, 'b', label='kalman position')\n",
    "plt.fill_between(range(len(predictions_df)),\n",
    "                 predictions_df.mu + predictions_df.sigma,\n",
    "                 predictions_df.mu - predictions_df.sigma, color='b', alpha=0.2)\n",
    "plt.fill_between(range(len(predictions_df)),\n",
    "                 predictions_df.mu + 3 * predictions_df.sigma,\n",
    "                 predictions_df.mu - 3 * predictions_df.sigma, color='b', alpha=0.1)\n",
    "plt.legend(loc='upper center', ncol=3, bbox_to_anchor=(0.5, 1.0), frameon=True)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('position')\n",
    "plt.title('Kalman filtering of location data')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hqIULzkxQ5HQ"
   },
   "source": [
    "# Problem 5 [2p]\n",
    "If possible, write the solutionsin a markdown cell\n",
    "\n",
    "1. Find simple expressions for the following functions' derivatives with respect to vector $\\mathbf{x}$.\n",
    "    1. $\\tanh(\\mathbf{x})$\n",
    "    2. $\\sigma(\\mathbf{x}) = \\frac{1}{1 + e^{-\\mathbf{x}}}$\n",
    "2. Find the following functions' gradients with respect to vector $[x, y, z]^T$:\n",
    "    1. $f_1([x, y, z]^T) = x + y$\n",
    "    2. $f_2([x, y, z]^T) = xy$\n",
    "    3. $f_3([x, y, z]^T) = x^2y^2$\n",
    "    4. $f_4([x, y, z]^T) = (x + y)^2$\n",
    "    5. $f_5([x, y, z]^T) = x^4 + x^2 y z + x y^2 z + z^4$\n",
    "    6. $f_6([x, y, z]^T) = e^{x + 2y}$\n",
    "    7. $f_7([x, y, z]^T) = \\frac{1}{x y^2}$\n",
    "    8. $f_8([x, y, z]^T) = ax + by + c$\n",
    "    9. $f_9([x, y, z]^T) = \\tanh(ax + by + c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FvTIMFwwQ5HR"
   },
   "source": [
    "# Problem 6 [2p]\n",
    "If possible, write the solutionsin a markdown cell\n",
    "\n",
    "Find the following functions' gradients or Jacobians with respect to vector $\\mathbf{x}$, where $\\mathbf{x}, \\mathbf{b} \\in \\mathbb{R}^{n}$, $\\mathbf{W} \\in \\mathbb{R}^{n \\times n}$:\n",
    "\n",
    "1. **[0.5p]** $\\mathbf{W} \\mathbf{x} + \\mathbf{b}$\n",
    "2. **[0.5p]** $\\mathbf{x}^T \\mathbf{W} \\mathbf{x}$,\n",
    "3. **[0.5p]** $\\sigma(\\mathbf{W} \\mathbf{x} + \\mathbf{b})$,\n",
    "    with $\\sigma$ applied element-wise. Hint: use the Hadamard product\n",
    "    (denoted $\\circ$) for element-wise matrix multiplication.\n",
    "4. **[0.5p]** $-\\log(S(\\mathbf{x})_j)$, where $S$ is the\n",
    "    softmax function\n",
    "    (https://en.wikipedia.org/wiki/Softmax_function) and we are\n",
    "    interested in the derivative over the $j$-th output of the\n",
    "    Softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FzXCYMgmQ5HS"
   },
   "source": [
    "# Problem 7 [2p] Linear Regression\n",
    "\n",
    "1. Implement a function generating a dataset of $n$ points\n",
    "  according to the following algorithm:\n",
    "  1. Draw $n$ points $x \\propto U(0;10)$ (uniformly distributed on $[0,10]$).\n",
    "  2. Draw $n$ points $y \\propto \\mathcal{N}(1+20x-1.3x^2, 7)$\n",
    "    (from a Gaussian distribution with $\\mu=1+20x-1.3x^2$ and $\\sigma=7$).\n",
    "\n",
    "  Prepare a dataset of 30 elements and make a scatterplot of\n",
    "  the expected value $y$ in function $x$.\n",
    "\n",
    "2. Use linear regression to fit polynomials to the\n",
    "  generated dataset. Fit polynomials of degrees zero (a constant line),\n",
    "  one, two and three. An easy way to do it is to transform each data\n",
    "  point $x$ into a vector of its powers $[1, x, x^2, \\ldots, x^m]$.\n",
    "\n",
    "  Plot the dataset and all regression curves on one figure.\n",
    "\n",
    "  **Note:** The name _linear regression_ signifies that the\n",
    "  hypothesis is linear with respect to parameters $\\Theta$.\n",
    "  However, the relationship between $x$ and $y$ is not constrained\n",
    "  to a linear one. In this exercise it is a polynomial one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4E4U26MkQ5HT"
   },
   "outputs": [],
   "source": [
    "def make_dataset(N):\n",
    "    X = TODO\n",
    "    Y = TODO\n",
    "    return X,Y\n",
    "\n",
    "data = make_dataset(30)\n",
    "scatter(TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yB5_4nkuQ5HY"
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "def powers_of_X(X, degree):\n",
    "    powers = np.arange(degree + 1).reshape(-1,1)\n",
    "    return X**powers\n",
    "\n",
    "def compute_polynomial(X, Theta):\n",
    "    XP = powers_of_X(X, len(Theta) - 1) # len(Theta) x N\n",
    "    Y = TODO\n",
    "    return Y\n",
    "\n",
    "plot_x_space = np.linspace(0,10,100)\n",
    "scatter(data[0], data[1])\n",
    "for degree in range(4):\n",
    "    X = powers_of_X(data[0], degree) # Matrix d x N\n",
    "    Y = data[1].reshape(1, -1)       # Matrix 1 x N\n",
    "    Theta = TODO\n",
    "    plot(plot_x_space, compute_polynomial(plot_x_space, Theta).ravel(), \n",
    "         label=\"degree: %d\" %(degree, ))\n",
    "    print Theta, Theta.shape\n",
    "legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S6U9fsbSQ5He"
   },
   "source": [
    "# Problem 8 [1p]\n",
    "\n",
    "When the data set is small and highly dimensional (or when high degree polynomials are used) the linear regression solution may fit the noise in the data instead of capturing the general rule. We call this phenomenon overfitting and will discuss it in detail in a few lectures.\n",
    "\n",
    "One way of preventing overfitting is to force the model's parameters to be small. We call this *regularization*. Consider the following cost function:\n",
    "\n",
    "$$ J(\\Theta) = \\sum_{i=1}^N (y^{(i)} - \\Theta^T x^{(i)})^2 + \\frac{\\lambda}{2} \\Theta^T \\Theta $$\n",
    "\n",
    "Analyze datasets sampled using the following procedure:\n",
    "\n",
    "1. $x \\propto U(0;1)$: $x$ is sampled uniformly from the  $0-1$ range.\n",
    "2. $y \\propto \\mathcal{N}(\\mu=1+2x-5x^2 + 4x^3, \\sigma=0.1)$: then \n",
    "    $y$ is sampled from the Normal distribution with mean \n",
    "    $\\mu=1+2x-5x^2+4x^3$ and standard deviation $0.1$\n",
    "\n",
    "Repeat 30 times an experiment in which you sample a new training\n",
    "dataset, then fit polynomials of degree 0 to 14 and use $\\lambda$\n",
    "value from the set $\\{0, 10^{-6}, 10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}\\}$.\n",
    "\n",
    "Plot the mean training and testing errors. What is the effect of increasing $\\lambda$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cmYHwDlrQ5Hg"
   },
   "outputs": [],
   "source": [
    "def poly_fun(X, Theta):\n",
    "    \"\"\"Compute the value of polynomial with coefficients Theta for points in X\"\"\"\n",
    "    #\n",
    "    # TODO: Write body of the function.\n",
    "    # Return a vector of values in points from X (the same shape as X).\n",
    "    # \n",
    "\n",
    "#\n",
    "# The true polynomial relation:\n",
    "# y(x) = 1 + 2x -5x^2 + 4x^3\n",
    "#\n",
    "# TODO: write down the proper coefficients\n",
    "#\n",
    "true_poly = np.array([1., 2., -5, 4])\n",
    "\n",
    "def make_dataset(N, theta=true_poly, sigma=0.1):\n",
    "    \"\"\" Sample a dataset \"\"\"\n",
    "    X = np.random.rand(N)\n",
    "    Y = np.random.randn(N)*sigma + poly_fun(X, true_poly)\n",
    "    return X,Y\n",
    "\n",
    "\n",
    "train_data = make_dataset(30)\n",
    "XX = np.linspace(0,1,100)\n",
    "YY = poly_fun(XX, true_poly)\n",
    "scatter(train_data[0], train_data[1], label='train data', color='r')\n",
    "plot(XX, poly_fun(XX, true_poly), label='ground truth')\n",
    "legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w4Iw1AaXQ5Hp"
   },
   "outputs": [],
   "source": [
    "#please note: lambda is a reserved keyword in python, thus we use _lambda\n",
    "def poly_fit(data, degree, _lambda):\n",
    "    \"Fit a polynomial of a given degree and weight decay parameter C\"\n",
    "    powers = np.arange(degree + 1.0).reshape(-1,1)\n",
    "    X = data[0].reshape(1,-1)\n",
    "    Y = data[1].reshape(1,-1)\n",
    "    XX = X**powers\n",
    "    #\n",
    "    # TODO: implement the closed-form solution for Theta\n",
    "    #\n",
    "    # Please note that np.inv may be numerically unstable.\n",
    "    # It is better to use np.linalg.solve or even a QR decomposition.\n",
    "    #\n",
    "    Theta = TODO\n",
    "    return Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7EMSo9gbQ5Ht",
    "outputId": "c7c19094-84a8-4229-e42d-3a100b0d1909"
   },
   "outputs": [],
   "source": [
    "lambdas = [0.0, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "degrees  = arange(15)\n",
    "\n",
    "num_repetitions = 30\n",
    "num_samples = 30\n",
    "\n",
    "train_errors = np.zeros((len(lambdas), len(degrees)))\n",
    "test_errors = np.zeros((len(lambdas), len(degrees)))\n",
    "\n",
    "#sample a single dataset for all experiments\n",
    "test_data = make_dataset(num_samples)\n",
    "\n",
    "for repetition in xrange(num_repetitions):\n",
    "    #sample a new training dataset for this repetition\n",
    "    train_data = make_dataset(num_samples)\n",
    "    #scatter(data[0], data[1])\n",
    "    for degree_i, degree in enumerate(degrees):\n",
    "        for lambda_i, _lambda in enumerate(lambdas):\n",
    "            Theta = poly_fit(train_data, degree, _lambda)\n",
    "            #\n",
    "            # TODO: compute the mean training and test errors\n",
    "            #\n",
    "            train_errors[lambda_i, degree_i] += TODO\n",
    "            test_errors[lambda_i, degree_i] += TODO\n",
    "train_errors /=  num_repetitions\n",
    "test_errors /= num_repetitions\n",
    "\n",
    "figure(figsize=(10,20))\n",
    "for lambda_i, _lambda in enumerate(lambdas):\n",
    "    subplot(len(lambdas), 1, lambda_i+1)\n",
    "    plot(degrees, train_errors[lambda_i,:], label='train')\n",
    "    plot(degrees, test_errors[lambda_i,:],  label='test')\n",
    "    ylim(0,0.1)\n",
    "    title('lambda=%g'%(_lambda,))\n",
    "    grid(True)\n",
    "    legend()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Assignment2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
